{"cells":[{"metadata":{"id":"ifHjDFrjdHG3"},"cell_type":"markdown","source":"# Imports and Setup"},{"metadata":{"id":"tllei7HoNvjB","trusted":true},"cell_type":"code","source":"# Some basic imports that we will need for the data\nimport numpy as np\nimport pandas as pd\nimport math\n\n# some basic imports that we will need for ML\nimport torch\nimport torch.nn as nn \nimport torch.nn.functional as F\nimport torch.optim as optim\nimport tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"id":"OReb85BgJ3kL","trusted":true},"cell_type":"code","source":"# get the life step function that will be used to score submissions\n\ndef life_step(X):\n    \"\"\"Game of life step using generator expressions\"\"\"\n    nbrs_count = sum(np.roll(np.roll(X, i, 0), j, 1)\n                     for i in (-1, 0, 1) for j in (-1, 0, 1)\n                     if (i != 0 or j != 0))\n    return (nbrs_count == 3) | (X & (nbrs_count == 2))","execution_count":null,"outputs":[]},{"metadata":{"id":"Vkkdrn4zSsKm","outputId":"6198c3c0-eb0d-4ac7-ba51-57361ab8279c","trusted":true},"cell_type":"code","source":"# a simple display of a blinker, using the life step function\n\nX = np.array([[0, 0, 0, 0, 0],[0, 0, 0, 0, 0], [0, 1, 1, 1, 0], [0, 0, 0, 0, 0],[0, 0, 0, 0, 0],])\nprint(X)\n\nfor i in range(3):\n  X = life_step(X)\n  print(X)","execution_count":null,"outputs":[]},{"metadata":{"id":"_sjS8y3jNTOj","outputId":"9c73c066-0cab-472f-e39a-7644545ed48f","trusted":true},"cell_type":"code","source":"# let's get our data in a useable format\ntrain_file = pd.read_csv('/kaggle/input/conways-reverse-game-of-life-2020/train.csv')\ntest_file = pd.read_csv('/kaggle/input/conways-reverse-game-of-life-2020/test.csv')\n\n# we should have 50,000 training and test games\n# each training game has an id, a delta (number of time steps), \n# 625 starting points, and 625 ending points, for a total of 1252 in each row\n# the test data is the same, except without the starting points\n# so there are 627 total elements in each row\nprint(train_file.shape)\nprint(test_file.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"3Ck-i9VEOUhI","outputId":"1eb28000-ae0d-4cf1-bf9b-7167fff13bc8","trusted":true},"cell_type":"code","source":"# let's take a peak at our training data\ntrain_file.iloc[0:4, [0, 1, 2, 3, 626, -624, -3, -2, -1]]","execution_count":null,"outputs":[]},{"metadata":{"id":"N3eTjf02OiXb","outputId":"b063c264-becd-46a2-bfe4-0683a40337e7","trusted":true},"cell_type":"code","source":"# and a quick peak at our test data\ntest_file.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]","execution_count":null,"outputs":[]},{"metadata":{"id":"Ge1xpcmfBF6H","trusted":true},"cell_type":"code","source":"# get just the data\n# drop the id and delta\ntrain_data = train_file.drop([\"id\", \"delta\"], axis = 1)\n\n# drop the start game data\ntrain_data.drop(train_data.columns[train_data.columns.str.startswith('start_')], axis = 1, inplace = True)\n\n# turn it into an array of arrays\ntrain_data = train_data.to_numpy()\n\n# get the starts of the test data\ntrain_starts = train_file.drop([\"id\", \"delta\"], axis = 1)\ntrain_starts.drop(train_starts.columns[train_starts.columns.str.startswith('stop_')], axis = 1, inplace = True)\ntrain_starts = train_starts.to_numpy()\n\n# get the deltas of each\ntrain_deltas = train_file[['delta']].to_numpy()\n\n# do the same for the test data\ntest_data = test_file.drop([\"id\", \"delta\"], axis = 1)\ntest_data = test_data.to_numpy()\n\n# make train sets and test sets\n# 0 will be the deltas; the rest will be the data\ntest_sets = test_file.drop([\"id\"], axis = 1).to_numpy()\ntrain_sets = train_file.drop([\"id\"], axis = 1).to_numpy() # 0 will be the deltas; the rest will be the data\n# the training data will be shuffled for random sampling\nnp.random.shuffle(train_sets)","execution_count":null,"outputs":[]},{"metadata":{"id":"rWqUIYojks9z","outputId":"84d1a119-01dc-4a21-d8fd-42b0305fec4b","trusted":true},"cell_type":"code","source":"# example of unpacking data\ndelta = train_sets[0][0]\nstart = train_sets[0][1:626]\nend = train_sets[0][626:]\nprint(\"delta: %d\" % delta)\nprint(\"start shape: %d\" % start.shape)\nprint(\"end shape: %d\" % end.shape)\n\ndelta = test_sets[0][0]\nend = test_sets[0][1:]\nprint(\"delta: %d\" % delta)\nprint(\"end shape: %d\" % end.shape)","execution_count":null,"outputs":[]},{"metadata":{"id":"aUBByP8-XAbU","trusted":true},"cell_type":"code","source":"# create a function to find if a starting setup is correct\n# for a given ending setup\n\n# This didn't get used in the end for two reason:\n# first, it requires a gpu kernel that is currently under development. Rolls are currently converted to np arrays, which is a problem.\n# second, I'm not confident enough in my ability to make use of gradients to modify them as I pass in tensors through this \"layer\"\n\ndef getError(startGame, endGame, numTimeSteps):\n  dim = round(math.sqrt(len(startGame)))\n  currentGame = startGame.reshape((dim, dim))\n  endGame = endGame.reshape((dim, dim))\n\n  for i in range(numTimeSteps):\n    currentGame = life_step(currentGame)\n  \n  error = np.sum((currentGame - endGame) ** 2)\n  \n  return error\n","execution_count":null,"outputs":[]},{"metadata":{"id":"_UH45AakYIde","outputId":"f66c4db1-0942-4e80-f0af-bdb33f0c198d","trusted":true},"cell_type":"code","source":"# test to make sure that my error function is happy with all of their\n# training data\n# the error should be 0 in the end\ntotalError = 0\n\nfor i in range(len(train_data)):\n  startGame = train_starts[i]\n  endGame = train_data[i]\n  timeStep = train_deltas[i]\n  totalError += getError(startGame, endGame, timeStep[0])\n\ntotalError","execution_count":null,"outputs":[]},{"metadata":{"id":"fOKnw7gqeY3K"},"cell_type":"markdown","source":"# Neural Network for the forward Direction\n\nThe game of life is a lot simpler forwards - afterall, it is deterministic!\n\nConsider this kernel:\n\n1 1 1\n\n1 9 1\n\n1 1 1\n\nIf the result is 3, 11, or 12, the cell in the next time step should be alive.\nIf the result is anything else, it should be dead.\n\n3 Represents a dead cell, touching exaclty 3 living cells - bring it to life!\n11 and 12 represent a living cell touching 2 (11) or 3 (12) other living cells,\nso this cell should remain alive in the next time step.\n\nSo this simple neural network should handle one time step perfectly!\n\nI did some reading here, though mine is *slightly* different. There are a number of ways to setup this convolution \"pefectly\":\n\nhttps://medium.com/@tomgrek/evolving-game-of-life-neural-networks-chaos-and-complexity-94b509bc7aa8\n\nThe idea, if I can get it working, is to use this to make something similar to (though not quite the same as) an autoencoder. This will allow me to make a neural network that can predict the start state, then, using this neural network as essentially one of the layers, it will use that start state to generate an end state. Both of the prediction of the start state and the end state will both generate a loss that I can use to train my model.\n\nWhile I was closer to getting this working than the above error function, I still couldn't get it working in the end. :( The reshaping, gradient handling, etc, was just beyond me."},{"metadata":{"id":"t1sVW8S-wm2c","trusted":true},"cell_type":"code","source":"# Let's setup a neural network on a cuda device\ndevice = torch.device('cuda')","execution_count":null,"outputs":[]},{"metadata":{"id":"T6U8rBB7e8i5","trusted":true},"cell_type":"code","source":"class forwardGame(nn.Module):\n  def __init__(self):\n    super().__init__()\n    self.step = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding=1, padding_mode='circular')\n    # let's set that weight!\n    # I've never done this before, but it should be similar to this:\n    # https://discuss.pytorch.org/t/setting-custom-kernel-for-cnn-in-pytorch/27176/2\n    with torch.no_grad():\n      self.step.weight = nn.Parameter(torch.tensor([[1, 1, 1], [1, 9, 1], [1, 1, 1]]).type('torch.FloatTensor').view(1, 1, 3, 3).repeat(1, 1, 1, 1))\n    \n    # this layer doesn't need to change... so don't let it!\n    for param in self.parameters():\n      self.requires_grad = False\n  \n  def forward(self, x, delta):\n    \n    # make one step for each number in the series\n    for i in range(delta):\n      # round, since we won't always be dealing with pretty numbers\n      # e.g. when we reconstruct, they will a variety of numbers\n      x = self.step(x).round()\n      # this is the magic; described in text above\n      # turn it from bool to int to float, so it can be convoluted again\n      x = (((x == 3) | (x == 11) | (x == 12)).int()).type('torch.FloatTensor')\n    \n    return x","execution_count":null,"outputs":[]},{"metadata":{"id":"UiPoX6Umiwpf","trusted":true},"cell_type":"code","source":"def prepForConv(x):\n  dim = round(math.sqrt(len(x)))\n  return torch.from_numpy(x).type('torch.FloatTensor').reshape((dim, dim))[None, None, ...]","execution_count":null,"outputs":[]},{"metadata":{"id":"8fw9T5VmhpSv","outputId":"2002271f-8c47-45fb-bbcb-5ff658309f11","trusted":true},"cell_type":"code","source":"# let's give it a whirl!\nforwardCNN = forwardGame()\n\n# this is our blinker from before. It should alternate between a vertical and horizontal line\nstart = prepForConv(np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n\nprint(start)\nfor i in range(3):\n  start = forwardCNN(start, 1)\n  print(start)","execution_count":null,"outputs":[]},{"metadata":{"id":"-kutl9VqdCaf"},"cell_type":"markdown","source":"# Neural Network\n\n"},{"metadata":{"id":"obkWHHVgffjD","trusted":true},"cell_type":"code","source":"'''\n# first, not so successful attempt\nmodel = nn.Sequential(\n    nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding_mode = 'circular'),\n    nn.ReLU(),\n    nn.Conv2d(in_channels=1, out_channels=1, kernel_size=3, padding_mode = 'circular'),\n    nn.ReLU(),\n    nn.Flatten(),\n    nn.Linear(in_features = (441), out_features = 625),\n    nn.ReLU(),\n    nn.Linear(in_features = 625, out_features = 625),\n    nn.ReLU(),\n    nn.Linear(in_features = 625, out_features = 625),\n    nn.ReLU(),\n    nn.Linear(in_features = 625, out_features = 625),\n    \n).to(device)\n'''\n\nclass oneBack(nn.Module):\n  # I decided to use 512 channels this time, like the VGG16 model\n  # it starts with 256 channels, and ends with it, so it can be repeated\n  def __init__(self, channels = 256, mid_channels = 512):\n    super().__init__()\n    # 4 padding should keep each layer the same size\n    # padding is circular because the board wraps around by problem definition\n    self.convIn = nn.Conv2d(channels, mid_channels, 5, padding=2, padding_mode = 'circular')\n    self.convMid = nn.Conv2d(mid_channels, mid_channels, 5, padding=2, padding_mode = 'circular')\n    self.convOut = nn.Conv2d(mid_channels, channels, 5, padding=2, padding_mode = 'circular')\n    self.actv = nn.ReLU()\n  \n  def forward(self, x):\n    x = self.actv(self.convIn(x))\n    x = self.actv(self.convMid(x))\n    x = self.actv(self.convOut(x))\n    return x\n\nclass reverseTime(nn.Module):\n  def __init__(self, channels = 256, mid_channels = 512):\n    super().__init__()\n    self.actv = nn.ReLU()\n    self.convIn = nn.Conv2d(1, channels, 5, padding=2, padding_mode = 'circular')\n    self.oneBack = oneBack(channels, mid_channels)\n    self.convOut = nn.Conv2d(channels, 1, 5, padding=2, padding_mode = 'circular')\n    self.toLinear = nn.Flatten()\n    self.fc = nn.Linear(in_features = 625, out_features = 625)\n    self.toStart = nn.Linear(in_features = 625, out_features = 625)\n    self.finalActv = nn.Sigmoid()\n    self.rebuild = forwardGame()\n  \n  def forward(self, x, delta):\n    # first, get x ready to reverse back one step at a time\n    x = self.actv(self.convIn(x))\n\n    # take it back, one time step at a time\n    for i in range(delta):\n      x = self.oneBack(x)\n    \n    # transform it back to one layer of 625\n    x = self.actv(self.convOut(x))\n\n    # flatten it to get the result\n    x = self.actv(self.toLinear(x))\n    \n    x = self.actv(self.fc(x))\n\n    # get start\n    x = self.actv(self.toStart(x))\n\n    #y = self.rebuild(x.reshape((25, 25))[None, None, ...].to(device), delta)\n\n    return x #, self.toLinear(y)\n\nmodel = reverseTime().to(device)","execution_count":null,"outputs":[]},{"metadata":{"id":"Tb73BTaBgTLt","trusted":true},"cell_type":"code","source":"# use MSE Loss to evaluate how well it did overall\n# first, failed attempt\n#criterion = nn.MSELoss()\n\n# binary logistic loss seems to make a lot more sense\n# especially since we are dealing with 0's and 1's\ncriterion = nn.BCEWithLogitsLoss()\n\n# hyperparameters\n\nbatch_size = 50\nepochs = 100\nlearning_rate = 0.005\n\n# set up the optimizer\n\noptimizer = torch.optim.Adam(\n    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n\ntrain_iterator = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"id":"bwbtm6gzexkL","trusted":true},"cell_type":"code","source":"# setup a basic training function\ndef train(model, device, train_data, optimizer, criterion, ):\n    \n    epoch_loss = 0\n    \n    model.train()\n    \n    num_tests = 0\n    \n    losses = []\n    \n    for x in train_data:\n        delta = x[0]\n        x_start = x[1:626]\n        x_end = x[626:]\n        dim = round(math.sqrt(len(x_start)))\n        x_end_t = torch.from_numpy(x_end).type('torch.FloatTensor').reshape((dim, dim))[None, None, ...].to(device)\n        x_start_t = torch.from_numpy(x_start).type('torch.FloatTensor')[None, ...].to(device)\n        \n    \n        optimizer.zero_grad()\n        \n        pred_start = model(x_end_t, delta)\n\n        loss = criterion(pred_start, x_start_t)\n        \n        \n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        losses.append(loss.item())\n        num_tests += 1\n        \n    return epoch_loss / num_tests, losses\n\n# setup a basic validation function\ndef evaluate(model, device, train_data, optimizer, criterion):\n    \n    epoch_loss = 0\n    losses = []\n    \n    model.eval()\n    \n    num_tests = 0\n    with torch.no_grad():\n        for x in train_data:\n            delta = x[0]\n            x_start = x[1:626]\n            x_end = x[626:]\n            dim = round(math.sqrt(len(x_start)))\n            x_end_t = torch.from_numpy(x_end).type('torch.FloatTensor').reshape((dim, dim))[None, None, ...].to(device)\n            x_start_t = torch.from_numpy(x_start).type('torch.FloatTensor')[None, ...].to(device)\n\n            pred_start = model(x_end_t, delta)\n\n            loss = criterion(pred_start, x_start_t)\n            \n            epoch_loss += loss.item()\n            losses.append(loss.item())\n            num_tests += 1\n\n    return epoch_loss / num_tests, losses","execution_count":null,"outputs":[]},{"metadata":{"id":"7YfrIvn7htqg","trusted":true},"cell_type":"code","source":"# set up a basic prediction function\n# this is what will take the input, and give the final output to submit\ndef predict(model, device, val_data):\n    \n    results = []\n\n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n        for x in val_data:\n\n            delta = x[0]\n            curr_game = x[1:626]\n            dim = round(math.sqrt(len(curr_game)))\n            curr_game = torch.from_numpy(curr_game)\n            \n            curr_game = curr_game.type('torch.FloatTensor').reshape((dim, dim))[None, None, ...].to(device)\n            curr_game = model(curr_game, delta).round()\n            \n            results.append(curr_game[0].detach().cpu().numpy())\n\n\n    return results","execution_count":null,"outputs":[]},{"metadata":{"id":"fzBBaUtDko8F"},"cell_type":"markdown","source":"# Train the model"},{"metadata":{"id":"sMAcY7OxkrLb","outputId":"205a48b1-bd90-4050-8955-dd64e1d91908","trusted":true},"cell_type":"code","source":"# this is essentially the batch size\nnum_tensors_per_epoch = round(len(train_sets) / epochs)\nnum_tensors_for_valid = round(num_tensors_per_epoch * .2)\n\nallLosses = []\nallValLosses = []\n\nfor epoch in range(epochs):\n  start = num_tensors_per_epoch * epoch\n  end = num_tensors_per_epoch * (epoch + 1)\n  train_loss, individualLosses = train(model, device, train_sets[start:(end-num_tensors_for_valid)], optimizer, criterion)\n  val_loss, individualValLosses = evaluate(model, device, train_sets[(end - num_tensors_for_valid):end], optimizer, criterion)\n  print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Validation Loss: {val_loss:.3f}')\n  allLosses += individualLosses\n  allValLosses += individualValLosses","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"First set, training loss:\", allLosses[0])\nprint(\"First set, Val loss:\", allValLosses[0])\nprint(\"All training loss: \", allLosses)\nprint(\"All validation loss: \", allValLosses)","execution_count":null,"outputs":[]},{"metadata":{"id":"oLA0lOEtKXY9","trusted":true},"cell_type":"code","source":"# make the necessary predictions\npredictions = predict(model, device, test_sets)","execution_count":null,"outputs":[]},{"metadata":{"id":"nQTp0YaVhsaj","trusted":true},"cell_type":"code","source":"# convert to an np array (use this as data in df)\nnp_pred = np.array(predictions)\nprint(np_pred)","execution_count":null,"outputs":[]},{"metadata":{"id":"4EMmuwMIeN-r","trusted":true},"cell_type":"code","source":"# convert to a df for easy submission\nanswer = pd.DataFrame(data = np_pred, columns = [\"start_%d\" % i for i in range(625)])\n\nanswer.insert(0, 'id', answer.index + 50000)\n\nanswer.to_csv('submission.csv', index=False)\nanswer","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}